{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a16baf78-070b-4409-a135-2fec83eea499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ritu\n"
     ]
    }
   ],
   "source": [
    "print('Ritu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ebc75a-2e91-4f57-9b13-ecee909ffa3b",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "Ensemble Learning is a technique that combines multiple machine learning models to improve performance, accuracy, and robustness. Instead of relying on a single model, ensemble methods aggregate predictions from multiple models to reduce bias, variance, and improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb14152-d213-40df-af6e-b9dc9f044263",
   "metadata": {},
   "source": [
    "# Types of Ensemble Learning\n",
    "1. Bagging (Bootstrap Aggregating)\n",
    "2. Boosting\n",
    "3. Stacking\n",
    "4. Voting & Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37129c05-c532-4cab-9819-74ba6fbdb9fd",
   "metadata": {},
   "source": [
    "# Wisdom of the Crowd\n",
    "The Wisdom of the Crowd is a phenomenon where collective decisions made by a group of individuals are often more accurate or reliable than those made by a single expert. This principle is widely used in ensemble learning, recommendation systems, and crowd-sourced intelligence."
   ]
  },
  {
   "cell_type": "raw",
   "id": "aae3ddf3-cbef-4bde-83ae-911d30789b60",
   "metadata": {},
   "source": [
    "Advantages and Disadvantages of Ensemble Learning\n",
    "‚úÖ Advantages of Ensemble Learning\n",
    "1Ô∏è‚É£ Higher Accuracy\n",
    "2Ô∏è‚É£ Reduces Overfitting (Variance Reduction)\n",
    "3Ô∏è‚É£ Works with Weak Models (Bias Reduction)\n",
    "4Ô∏è‚É£ Handles Complex Problems Well\n",
    "5Ô∏è‚É£ Robustness to Noisy Data\n",
    "\n",
    "‚ùå Disadvantages of Ensemble Learning\n",
    "1Ô∏è‚É£ Computationally Expensive\n",
    "2Ô∏è‚É£ Hard to Interpret\n",
    "3Ô∏è‚É£ Difficult Hyperparameter Tuning\n",
    "4Ô∏è‚É£ May Not Always Improve Performance\n",
    "\n",
    "üìå When to Use Ensemble Learning?\n",
    "‚úÖ When accuracy is more important than speed.\n",
    "‚úÖ When models overfit and need regularization.\n",
    "‚úÖ When handling noisy or high-dimensional data.\n",
    "\n",
    "üî¥ Avoid ensemble learning when: ‚ùå You need a simple, interpretable model.\n",
    "‚ùå Computational resources are limited.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7708d82-b1da-47c9-ad95-9f8022b90760",
   "metadata": {},
   "source": [
    "# Voting Ensemble Learning\n",
    "Voting ensemble learning is a technique that combines multiple models (classifiers or regressors) to improve accuracy. The final prediction is made based on the majority vote (hard voting) or the average probability (soft voting) from all models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ed21b-9450-465f-9e2a-0c3c4bc7e6b6",
   "metadata": {},
   "source": [
    "# Types of Voting in Ensemble Learning\n",
    "1Ô∏è‚É£ Hard Voting (Majority Voting)\n",
    "* Each model predicts a class label.\n",
    "* The final prediction is the most frequent class.\n",
    "\n",
    "2Ô∏è‚É£ Soft Voting (Probability Averaging)\n",
    "* Each model outputs class probabilities.\n",
    "* The final prediction is based on the highest average probability.\n",
    "\n",
    "üìå Voting in Regression\n",
    "‚úÖ Only Soft Voting is used for regression because regression models predict continuous values, not class labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b5b9ef-f164-4af2-bf43-ce642f14a9a6",
   "metadata": {},
   "source": [
    "# Disadvantages\n",
    "* ‚ùå Computationally Expensive ‚Üí Needs multiple models to be trained.\n",
    "* ‚ùå Not Always Better ‚Üí If base models are weak, ensemble won‚Äôt help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87ea9d57-a068-4155-8d95-10453c5f31ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ritu\n"
     ]
    }
   ],
   "source": [
    "print('Ritu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afc4cd2-2fed-4348-95d0-fd51537fffb6",
   "metadata": {},
   "source": [
    "# Bagging \n",
    "Bagging (Bootstrap Aggregating) is an ensemble learning technique in machine learning that improves model accuracy and reduces variance by training multiple models on different subsets of the data and averaging their predictions.\n",
    "\n",
    "## How Bagging Works:\n",
    "1. <b>Bootstrap Sampling</b>: Multiple subsets of the original dataset are created using random sampling with replacement.\n",
    "2. <b>Training Multiple Models</b>: A separate model (usually the same type) is trained on each subset independently.\n",
    "3. <b>Aggregation</b>:\n",
    "     * For regression: The average of all predictions is taken.\n",
    "    * For classification: The majority vote (mode) is used.\n",
    "  \n",
    "## Advantages of Bagging:\n",
    "* Reduces Overfitting: Since multiple models are used, bagging reduces variance and helps prevent overfitting.\n",
    "* Improves Stability & Accuracy: Reducing variance makes the model more robust and improves predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68306d78-e9c2-4f60-b5db-a8b75a1cd016",
   "metadata": {},
   "source": [
    "## How Bagging Achieves LBLV\n",
    "* <b>Low Bias</b>: Bagging typically uses high-variance models like decision trees (which have low bias but high variance). Since individual models are complex, they still capture patterns well.\n",
    "* <b>Low Variance</b>: Bagging reduces variance by averaging predictions from multiple models trained on different subsets of data. Since each model is trained on slightly different data, their errors cancel out when aggregated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c613c1a-7c37-44e1-833f-c0a65dd350c4",
   "metadata": {},
   "source": [
    "# What is OOB?\n",
    "* In Bootstrap Aggregating (Bagging), each base model (e.g., Decision Tree) is trained on a random sample (with replacement) from the original dataset.\n",
    "* On average, each sample leaves out about 37% of the original data (i.e., \"Out-of-Bag\" samples).\n",
    "* These OOB samples can be used to test the model‚Äôs performance without requiring a separate validation/test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c08043-5fa4-4b2b-bbcd-db4710be2ec4",
   "metadata": {},
   "source": [
    "# Difference Between Bagging and Random Forest\n",
    "1. <b>Base Model</b>:\n",
    "* Can be any model (e.g., Decision Trees, SVM, Neural Networks, etc.).\n",
    "* Always uses Decision Trees as the base model.\n",
    "2. <b>Feature Selection</b>:\n",
    "* Uses all features for training each model.\n",
    "* Selects a random subset of features at each node split, reducing correlation between trees.\n",
    "* If Bagging uses feature sampling, then it selects features only for the root split, not for node splits, and uses the same subset for all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108de6e3-0043-4c88-b780-65e65bfc66b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
